{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=7, micro=9, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a65f97fcad91>:11: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# --- Uncomment to use only CPU (e.g. GPU memory is too small)\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/usr/local/cuda-10.1/bin\")\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-10.1/lib64\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Old stuff:\n",
    "\n",
    "import pickle\n",
    "\n",
    "def process_sample(sample, sample_name):\n",
    "    force_power, force_angle = [float(x) for x in sample_name.split(\"_\")]\n",
    "    result = list()\n",
    "    for n in range(240):\n",
    "        if n == 5:\n",
    "            frame_power = force_power\n",
    "        else:\n",
    "            frame_power = 0\n",
    "        result.append(flatten_frame(sample[n], force_power, force_angle))\n",
    "    return result\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"data\")\n",
    "models_dir = os.path.join(os.getcwd(), \"model\")\n",
    "\n",
    "data = list()\n",
    "files = os.listdir(data_dir)\n",
    "for num, fname in enumerate(files):\n",
    "    if not num % 10:\n",
    "        print(num, \"/\", len(files), end=\"\\r\")\n",
    "    with open(os.path.join(data_dir, fname), \"rb\") as f:\n",
    "        data.append(process_sample(pickle.load(f, encoding='latin1'), fname.rsplit(\".\", 1)[0]))\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "for num, (values, sample) in enumerate(data.items()):\n",
    "    if not num % 10:\n",
    "        print(num, \"/\", len(files), end=\"\\r\")\n",
    "    force_power, force_angle = [float(x) for x in values.split(\"_\")]\n",
    "    for n in range(240 - 4):\n",
    "        if n == 5:\n",
    "            frame_power = force_power\n",
    "        else:\n",
    "            frame_power = 0\n",
    "        x.append(flatten_frame(sample[n], force_power, force_angle) +\n",
    "                 flatten_frame(sample[n+1], force_power, force_angle) + \n",
    "                 flatten_frame(sample[n+2], force_power, force_angle))\n",
    "        y.append(flatten_frame(sample[n+3], force_power, force_angle)[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:24:00,103: Logger started.\n"
     ]
    }
   ],
   "source": [
    "# reate logger - nocer formatting\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging._warn_preinit_stderr = 0\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s: %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.handlers = [ch]\n",
    "logger.info(\"Logger started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import numpy as np\n",
    "\n",
    "def adapt_array(arr):\n",
    "    \"\"\"\n",
    "    Save Numpy array to SqLite.\n",
    "    Source:\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(out.read())\n",
    "\n",
    "def convert_array(text):\n",
    "    \"\"\" \n",
    "    Load Numpy array from Sqlite.\n",
    "    Source:\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    return np.load(out)\n",
    "\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "data_path = os.path.join(os.getcwd(), \"samplesGeneration\", \"prepared_data.bd\")\n",
    "source = sqlite3.connect(data_path, detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread=False)\n",
    "db = sqlite3.connect(':memory:')\n",
    "source.backup(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# --- Uncomment to use only CPU (e.g. GPU memory is too small)\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/usr/local/cuda-10.1/bin\")\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-10.1/lib64\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  1703921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x23134432420>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "db_cursor = db.cursor()\n",
    "sql_query = \"SELECT COALESCE(MAX(id)+1, 0) FROM data\"\n",
    "db_cursor.execute(sql_query)\n",
    "number_of_samples = db_cursor.fetchone()[0]\n",
    "print(\"number of samples: \", number_of_samples)\n",
    "\n",
    "sql_query = \"SELECT x, y FROM data WHERE id == 1\"\n",
    "db_cursor.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras from SQLite database'\n",
    "    def __init__(self, path, indexes=None, batch_size=25,\n",
    "                 shuffle=True, dim=(117, 1), dim_y=(114, 1), column=\"voxels\"):\n",
    "        'Initialization'\n",
    "        if indexes is None:\n",
    "            raise Exception(\"Indexes need to be provided!\")\n",
    "        sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "        sqlite3.register_converter(\"array\", convert_array)\n",
    "        self.path = path\n",
    "        self.dim = dim\n",
    "        self.dim_y = dim_y\n",
    "        self.db = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread=False)\n",
    "        self.db_cursor = self.db.cursor()\n",
    "        self.N = len(indexes) - 1\n",
    "        self.sample_index = indexes\n",
    "        self.column = column\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.db.close()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.N / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data - generates indexes of the batch'\n",
    "        # Generate indexes of the batch\n",
    "        samples_batch = np.arange((index) * self.batch_size, (index+1) * self.batch_size)\n",
    "\n",
    "        # Generate data\n",
    "        while True:\n",
    "            try:\n",
    "                x, y = self.__data_generation(samples_batch)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return  x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.sample_index)\n",
    "\n",
    "    def __data_generation(self, samples_batch):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        x = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty(shape=(self.batch_size, *self.dim_y))\n",
    "        inds = self.sample_index[samples_batch]\n",
    "        sqlite3.register_adapter(np.ndarray, self.adapt_array)\n",
    "        sqlite3.register_converter(\"array\", self.convert_array)\n",
    "        db = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "        db_cursor = db.cursor()\n",
    "        sql_query = \"SELECT x, y FROM data WHERE id in ({index})\".\\\n",
    "            format(column=self.column, index=','.join(str(ind) for ind in inds))\n",
    "        db_cursor.execute(sql_query)\n",
    "        try:\n",
    "            for i, line in zip(range(self.batch_size), db_cursor.fetchall()):\n",
    "                if line is None:\n",
    "                    x[i, :] = x[i - 1, :]\n",
    "                    y[i] = y[i - 1]\n",
    "                    continue # Bad, temporary solution\n",
    "                x[i, :] = line[0].reshape(self.dim)\n",
    "                y[i] = line[1].reshape(self.dim_y)\n",
    "        except TypeError as e:\n",
    "            print(sql_query)\n",
    "            raise e\n",
    "        # x.reshape(self.batch_size, *self.dim, 1)\n",
    "        y = y.reshape(self.batch_size, *self.dim_y)\n",
    "        return x, y\n",
    "    \n",
    "    def adapt_array(self, arr):\n",
    "        \"\"\"\n",
    "        Save Numpy array to SqLite.\n",
    "        Source:\n",
    "        http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "        \"\"\"\n",
    "        out = io.BytesIO()\n",
    "        np.save(out, arr)\n",
    "        out.seek(0)\n",
    "        return sqlite3.Binary(out.read())\n",
    "\n",
    "    def convert_array(self, text):\n",
    "        \"\"\" \n",
    "        Load Numpy array from Sqlite.\n",
    "        Source:\n",
    "        http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "        \"\"\"\n",
    "        out = io.BytesIO(text)\n",
    "        out.seek(0)\n",
    "        return np.load(out)\n",
    "    \n",
    "test_train_ratio = 0.1\n",
    "\n",
    "test_size = int(test_train_ratio * number_of_samples)\n",
    "\n",
    "indexes = np.arange(0, number_of_samples - 1)\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, val_indexes = indexes[:-test_size], indexes[test_size:]\n",
    "\n",
    "training_generator = DataGenerator(data_path, indexes=train_indexes, batch_size=32, dim=(117, 1), dim_y=(36, 1))\n",
    "validation_generator = DataGenerator(data_path, indexes=val_indexes, batch_size=32, dim=(117, 1), dim_y=(36, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "best_model_file = \"model.h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', mode='min',verbose=1, save_best_only=True)\n",
    "\n",
    "models_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:24:06,143: start\n",
      "2020-12-04 11:24:06,143: Create model...\n",
      "2020-12-04 11:24:06,768: Fit model..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 36)                4644      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 36, 1)             0         \n",
      "=================================================================\n",
      "Total params: 71,204\n",
      "Trainable params: 71,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-9-20e079fc1eb0>:32: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:24:06,768: From <ipython-input-9-20e079fc1eb0>:32: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:24:06,789: sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:24:06,831: sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 47922 steps, validate for 47922 steps\n",
      "Epoch 1/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.1811\n",
      "Epoch 00001: val_loss improved from inf to 0.07187, saving model to model.h5\n",
      "47922/47922 [==============================] - 1771s 37ms/step - loss: 0.1811 - val_loss: 0.0719\n",
      "Epoch 2/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.1027\n",
      "Epoch 00002: val_loss improved from 0.07187 to 0.04651, saving model to model.h5\n",
      "47922/47922 [==============================] - 1767s 37ms/step - loss: 0.1027 - val_loss: 0.0465\n",
      "Epoch 3/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.0955\n",
      "Epoch 00003: val_loss improved from 0.04651 to 0.04056, saving model to model.h5\n",
      "47922/47922 [==============================] - 1759s 37ms/step - loss: 0.0955 - val_loss: 0.0406\n",
      "Epoch 4/100\n",
      "47919/47922 [============================>.] - ETA: 0s - loss: 0.0933\n",
      "Epoch 00004: val_loss did not improve from 0.04056\n",
      "47922/47922 [==============================] - 1740s 36ms/step - loss: 0.0933 - val_loss: 0.0416\n",
      "Epoch 5/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.0921\n",
      "Epoch 00005: val_loss improved from 0.04056 to 0.03621, saving model to model.h5\n",
      "47922/47922 [==============================] - 1737s 36ms/step - loss: 0.0921 - val_loss: 0.0362\n",
      "Epoch 6/100\n",
      "47921/47922 [============================>.] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00006: val_loss did not improve from 0.03621\n",
      "47922/47922 [==============================] - 1736s 36ms/step - loss: 0.0914 - val_loss: 0.0387\n",
      "Epoch 7/100\n",
      "47919/47922 [============================>.] - ETA: 0s - loss: 0.0909\n",
      "Epoch 00007: val_loss did not improve from 0.03621\n",
      "47922/47922 [==============================] - 1738s 36ms/step - loss: 0.0909 - val_loss: 0.0382\n",
      "Epoch 8/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.0904\n",
      "Epoch 00008: val_loss improved from 0.03621 to 0.03434, saving model to model.h5\n",
      "47922/47922 [==============================] - 1737s 36ms/step - loss: 0.0904 - val_loss: 0.0343\n",
      "Epoch 9/100\n",
      "47920/47922 [============================>.] - ETA: 0s - loss: 0.0901\n",
      "Epoch 00009: val_loss did not improve from 0.03434\n",
      "47922/47922 [==============================] - 1741s 36ms/step - loss: 0.0901 - val_loss: 0.0398\n",
      "Epoch 10/100\n",
      "47919/47922 [============================>.] - ETA: 0s - loss: 0.0897\n",
      "Epoch 00010: val_loss did not improve from 0.03434\n",
      "47922/47922 [==============================] - 1727s 36ms/step - loss: 0.0897 - val_loss: 0.0356\n",
      "Epoch 11/100\n",
      "47921/47922 [============================>.] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00011: val_loss did not improve from 0.03434\n",
      "47922/47922 [==============================] - 1735s 36ms/step - loss: 0.0895 - val_loss: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 16:43:56,564: Summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 36)                4644      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 36, 1)             0         \n",
      "=================================================================\n",
      "Total params: 71,204\n",
      "Trainable params: 71,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-20e079fc1eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_and_train_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-20e079fc1eb0>\u001b[0m in \u001b[0;36mcreate_and_train_network\u001b[1;34m(generator, val_generator, epohs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Summary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_all_points_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Try learning for all of verts:\n",
    "\n",
    "verts = range(14)\n",
    "pivot_vert = 3\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from collections import deque\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "\n",
    "models_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\", \"model_{}\".format(time.time()))\n",
    "\n",
    "\n",
    "def create_and_train_network(generator, val_generator, epohs=10):\n",
    "    log_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\", \"model_{}\".format(time.time()))\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    def create_model():\n",
    "        model = Sequential()\n",
    "        #model.add(LSTM(512, input_shape=generator.dim, return_sequences=True))\n",
    "        #model.add(Dropout(0.2))\n",
    "        model.add(LSTM(128, input_shape=generator.dim, return_sequences=False))\n",
    "        model.add(Dropout(0.4))\n",
    "        #model.add(Flatten())\n",
    "        model.add(Dense(np.prod(generator.dim_y), activation='linear'))\n",
    "        model.add(Reshape((np.prod(generator.dim_y), 1)))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        return model\n",
    "\n",
    "    logger.info(\"Create model...\")\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    logger.info(\"Fit model..\")\n",
    "    network = model.fit_generator(training_generator, verbose=1, validation_data=validation_generator, \n",
    "                                  epochs=epohs, callbacks=[early_stopping, best_model, tensorboard_callback])\n",
    "    logger.info(\"Summary\")\n",
    "    model.summary()\n",
    "    model.save(os.path.join(models_dir, \"model_all_points_{}\".format(sample_length)))\n",
    "    return network\n",
    "\n",
    "\n",
    "logger.info(\"start\")\n",
    "network = create_and_train_network(training_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-04 17:58:57,257: Assets written to: model\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "model = tf.keras.models.load_model(best_model_file)\n",
    "\n",
    "tf.saved_model.save(model, \"model\\\\saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.train' has no attribute 'Saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-416a2593f909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.train' has no attribute 'Saver'"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
