{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(60*60*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-55a1b07fcfc5>:11: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# --- Uncomment to use only CPU (e.g. GPU memory is too small)\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/usr/local/cuda-10.1/bin\")\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-10.1/lib64\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Old stuff:\n",
    "\n",
    "import pickle\n",
    "\n",
    "def process_sample(sample, sample_name):\n",
    "    force_power, force_angle = [float(x) for x in sample_name.split(\"_\")]\n",
    "    result = list()\n",
    "    for n in range(240):\n",
    "        if n == 5:\n",
    "            frame_power = force_power\n",
    "        else:\n",
    "            frame_power = 0\n",
    "        result.append(flatten_frame(sample[n], force_power, force_angle))\n",
    "    return result\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"data\")\n",
    "models_dir = os.path.join(os.getcwd(), \"model\")\n",
    "\n",
    "data = list()\n",
    "files = os.listdir(data_dir)\n",
    "for num, fname in enumerate(files):\n",
    "    if not num % 10:\n",
    "        print(num, \"/\", len(files), end=\"\\r\")\n",
    "    with open(os.path.join(data_dir, fname), \"rb\") as f:\n",
    "        data.append(process_sample(pickle.load(f, encoding='latin1'), fname.rsplit(\".\", 1)[0]))\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "for num, (values, sample) in enumerate(data.items()):\n",
    "    if not num % 10:\n",
    "        print(num, \"/\", len(files), end=\"\\r\")\n",
    "    force_power, force_angle = [float(x) for x in values.split(\"_\")]\n",
    "    for n in range(240 - 4):\n",
    "        if n == 5:\n",
    "            frame_power = force_power\n",
    "        else:\n",
    "            frame_power = 0\n",
    "        x.append(flatten_frame(sample[n], force_power, force_angle) +\n",
    "                 flatten_frame(sample[n+1], force_power, force_angle) + \n",
    "                 flatten_frame(sample[n+2], force_power, force_angle))\n",
    "        y.append(flatten_frame(sample[n+3], force_power, force_angle)[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 08:00:03,420: Logger started.\n"
     ]
    }
   ],
   "source": [
    "# reate logger - nocer formatting\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging._warn_preinit_stderr = 0\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s: %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.handlers = [ch]\n",
    "logger.info(\"Logger started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import numpy as np\n",
    "\n",
    "def adapt_array(arr):\n",
    "    \"\"\"\n",
    "    Save Numpy array to SqLite.\n",
    "    Source:\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    out = io.BytesIO()\n",
    "    np.save(out, arr)\n",
    "    out.seek(0)\n",
    "    return sqlite3.Binary(out.read())\n",
    "\n",
    "def convert_array(text):\n",
    "    \"\"\" \n",
    "    Load Numpy array from Sqlite.\n",
    "    Source:\n",
    "    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "    \"\"\"\n",
    "    out = io.BytesIO(text)\n",
    "    out.seek(0)\n",
    "    return np.load(out)\n",
    "\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "data_path = os.path.join(os.getcwd(), \"samplesGeneration\", \"prepared_data.bd\")\n",
    "data_path = \"E:/tmp/prepared_data_abs_not_full.bd\"\n",
    "source = sqlite3.connect(data_path, detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread=False)\n",
    "db = sqlite3.connect(':memory:')\n",
    "source.backup(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# --- Uncomment to use only CPU (e.g. GPU memory is too small)\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/usr/local/cuda-10.1/bin\")\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-10.1/lib64\"\n",
    "\n",
    "#import tensorflow as tf\n",
    "#tf.test.is_gpu_available(cuda_only=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  1682261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1bec5c6cc00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "\n",
    "db_cursor = db.cursor()\n",
    "sql_query = \"SELECT COALESCE(MAX(id)+1, 0) FROM data\"\n",
    "db_cursor.execute(sql_query)\n",
    "number_of_samples = db_cursor.fetchone()[0]\n",
    "print(\"number of samples: \", number_of_samples)\n",
    "\n",
    "sql_query = \"SELECT x, y FROM data WHERE id == 1\"\n",
    "db_cursor.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras from SQLite database'\n",
    "    def __init__(self, path, indexes=None, batch_size=25,\n",
    "                 shuffle=True, dim=(3, 39, 1), dim_y=(36, 1), column=\"voxels\"):\n",
    "        'Initialization'\n",
    "        if indexes is None:\n",
    "            raise Exception(\"Indexes need to be provided!\")\n",
    "        sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "        sqlite3.register_converter(\"array\", convert_array)\n",
    "        self.path = path\n",
    "        self.dim = dim\n",
    "        self.dim_y = dim_y\n",
    "        self.db = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread=False)\n",
    "        self.db_cursor = self.db.cursor()\n",
    "        self.N = len(indexes) - 1\n",
    "        self.sample_index = indexes\n",
    "        self.column = column\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.db.close()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.N / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data - generates indexes of the batch'\n",
    "        # Generate indexes of the batch\n",
    "        samples_batch = np.arange((index) * self.batch_size, (index+1) * self.batch_size)\n",
    "\n",
    "        # Generate data\n",
    "        while True:\n",
    "            try:\n",
    "                x, y = self.__data_generation(samples_batch)\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return  x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.sample_index)\n",
    "\n",
    "    def __data_generation(self, samples_batch):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        x = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty(shape=(self.batch_size, *self.dim_y))\n",
    "        inds = self.sample_index[samples_batch]\n",
    "        sqlite3.register_adapter(np.ndarray, self.adapt_array)\n",
    "        sqlite3.register_converter(\"array\", self.convert_array)\n",
    "        db = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "        db_cursor = db.cursor()\n",
    "        sql_query = \"SELECT x, y FROM data WHERE id in ({index})\".\\\n",
    "            format(column=self.column, index=','.join(str(ind) for ind in inds))\n",
    "        db_cursor.execute(sql_query)\n",
    "        try:\n",
    "            for i, line in zip(range(self.batch_size), db_cursor.fetchall()):\n",
    "                if line is None:\n",
    "                    x[i, :] = x[i - 1, :]\n",
    "                    y[i] = y[i - 1]\n",
    "                    continue # Bad, temporary solution\n",
    "                x[i, :] = np.array(np.array_split(line[0].reshape(self.dim), 3)).reshape(self.dim)\n",
    "                y[i] = line[1].reshape(self.dim_y)\n",
    "        except TypeError as e:\n",
    "            print(sql_query)\n",
    "            raise e\n",
    "        # x.reshape(self.batch_size, *self.dim, 1)\n",
    "        y = y.reshape(self.batch_size, *self.dim_y)\n",
    "        return x, y\n",
    "    \n",
    "    def adapt_array(self, arr):\n",
    "        \"\"\"\n",
    "        Save Numpy array to SqLite.\n",
    "        Source:\n",
    "        http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "        \"\"\"\n",
    "        out = io.BytesIO()\n",
    "        np.save(out, arr)\n",
    "        out.seek(0)\n",
    "        return sqlite3.Binary(out.read())\n",
    "\n",
    "    def convert_array(self, text):\n",
    "        \"\"\" \n",
    "        Load Numpy array from Sqlite.\n",
    "        Source:\n",
    "        http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n",
    "        \"\"\"\n",
    "        out = io.BytesIO(text)\n",
    "        out.seek(0)\n",
    "        return np.load(out)\n",
    "    \n",
    "test_train_ratio = 0.1\n",
    "\n",
    "test_size = int(test_train_ratio * number_of_samples)\n",
    "\n",
    "indexes = np.arange(0, number_of_samples - 1)\n",
    "np.random.shuffle(indexes)\n",
    "train_indexes, val_indexes = indexes[:-test_size], indexes[test_size:]\n",
    "\n",
    "training_generator = DataGenerator(data_path, indexes=train_indexes, batch_size=8, dim=(3, 39), dim_y=(1, 36))\n",
    "validation_generator = DataGenerator(data_path, indexes=val_indexes, batch_size=8, dim=(3, 39), dim_y=(1, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "best_model_file = \"model.h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', mode='min',verbose=1, save_best_only=True)\n",
    "\n",
    "models_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSaveCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        self.model.save(\"model\\\\improved_model_wip_abs2\", save_format='tf')\n",
    "my_cb = CustomSaveCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 09:39:14,276: start\n",
      "2020-12-31 09:39:14,281: Create model...\n",
      "2020-12-31 09:39:14,520: Fit model..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3, 39)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 36)                10944     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 36, 1)             0         \n",
      "=================================================================\n",
      "Total params: 10,944\n",
      "Trainable params: 10,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1879: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189246/189254 [============================>.] - ETA: 0s - loss: 9.5551e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 09:53:50,461: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 09:53:52,131: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 1589s 8ms/step - loss: 9.5551e-04 - val_loss: 8.4051e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.00084 to 0.00084, saving model to model.h5\n",
      "Epoch 2/10\n",
      "189251/189254 [============================>.] - ETA: 0s - loss: 8.4200e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 10:18:57,440: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 10:18:59,457: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 1518s 8ms/step - loss: 8.4200e-04 - val_loss: 8.4001e-04\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00084 to 0.00084, saving model to model.h5\n",
      "Epoch 3/10\n",
      "189248/189254 [============================>.] - ETA: 0s - loss: 8.4346e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 10:43:45,207: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 10:43:46,607: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 1480s 8ms/step - loss: 8.4346e-04 - val_loss: 8.4010e-04\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00084\n",
      "Epoch 4/10\n",
      "189248/189254 [============================>.] - ETA: 0s - loss: 8.4387e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 11:08:18,759: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 11:08:20,154: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 1475s 8ms/step - loss: 8.4387e-04 - val_loss: 8.3933e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00084 to 0.00084, saving model to model.h5\n",
      "Epoch 5/10\n",
      "189253/189254 [============================>.] - ETA: 0s - loss: 8.3949e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 12:33:11,034: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 12:33:12,426: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 5082s 27ms/step - loss: 8.3949e-04 - val_loss: 8.3910e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00084 to 0.00084, saving model to model.h5\n",
      "Epoch 6/10\n",
      "189245/189254 [============================>.] - ETA: 0s - loss: 8.4286e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 12:59:57,800: Found untraced functions such as lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 12:59:59,832: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189254/189254 [==============================] - 1633s 9ms/step - loss: 8.4286e-04 - val_loss: 8.4007e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00084\n",
      "Epoch 7/10\n",
      "  1941/189254 [..............................] - ETA: 13:02 - loss: 7.9213e-04"
     ]
    }
   ],
   "source": [
    "# Try learning for all of verts:\n",
    "\n",
    "verts = range(14)\n",
    "pivot_vert = 3\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from collections import deque\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "models_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\", \"model_{}\".format(time.time()))\n",
    "\n",
    "def special_loss(y_true, y_pred):\n",
    "    custom_loss=tf.add(tf.square(y_true-y_pred)/tf.add(tf.square(y_pred), 0.1), tf.keras.losses.MSE(y_true, y_pred))\n",
    "    return custom_loss    \n",
    "\n",
    "def loss(y_true,y_pred):\n",
    "    return K.sum(K.square(y_pred - y_true) * K.square(100 * y_true) + K.square(y_pred - y_true))\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "def create_and_train_network(generator, val_generator, epohs=10):\n",
    "    log_dir = os.path.join(os.getcwd(), \"samplesGeneration\", \"logs\", \"model_{}\".format(time.time()))\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    def create_model():\n",
    "        input_1 = Input(shape=generator.dim, name='input_1')\n",
    "        x = LSTM(np.prod(generator.dim_y), return_sequences=False, dropout=0.2)(input_1)\n",
    "        output_1 = Reshape((np.prod(generator.dim_y), 1))(x)\n",
    "        model = Model(inputs=[input_1], outputs=[output_1])\n",
    "        model.compile(loss=\"mse\", optimizer='rmsprop')\n",
    "        return model\n",
    "\n",
    "    logger.info(\"Create model...\")\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    logger.info(\"Fit model..\")\n",
    "    network = model.fit_generator(training_generator, verbose=1, validation_data=validation_generator, \n",
    "                                  epochs=epohs, callbacks=[my_cb, early_stopping, best_model])\n",
    "    logger.info(\"Summary\")\n",
    "    model.summary()\n",
    "    model.save(os.path.join(models_dir, \"model\"))\n",
    "    return network\n",
    "\n",
    "\n",
    "logger.info(\"start\")\n",
    "network = create_and_train_network(training_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 09:38:44,994: Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 09:38:46,309: Assets written to: model\\improved_model_wip2\\assets\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(best_model_file, custom_objects={\"loss\": loss})\n",
    "\n",
    "model.save(\"model\\\\improved_model_wip_abs2\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "sqlite3.register_adapter(np.ndarray, adapt_array)\n",
    "sqlite3.register_converter(\"array\", convert_array)\n",
    "db = sqlite3.connect(data_path, detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "db_cursor = db.cursor()\n",
    "sql_query = \"SELECT x, y FROM data WHERE id == 1\"\n",
    "db_cursor.execute(sql_query)\n",
    "data = db_cursor.fetchone()[0]\n",
    "print(data.shape)\n",
    "predicted= list()\n",
    "frames = np.array(np.array_split(data, 3))\n",
    "for f in frames:\n",
    "    predicted.append(f)\n",
    "\n",
    "for n in range(240):\n",
    "    _in  = np.concatenate((predicted[-3], predicted[-2], predicted[-1])).reshape(116, 1)\n",
    "    pred = model.predict(_in)\n",
    "    p = (n ==5) *10\n",
    "    np.append(pred, p)\n",
    "    np.append(pred, data[-2])\n",
    "    np.append(pred, data[-1])\n",
    "    print(pred.shape)\n",
    "    predicted.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d', xlim=(-2,2),ylim=(-2,2), zlim=(0, 10))\n",
    "\n",
    "xs = list()\n",
    "ys = list()\n",
    "zs = list()\n",
    "sizes = list()\n",
    "sample_to_plot = data[600]\n",
    "for num, f in enumerate(sample_to_plot):\n",
    "    for i in range(4):\n",
    "        v = [f[9*i], f[9*i + 1], f[9*i + 2]]\n",
    "        # print(v)\n",
    "        xs.append(v[0])\n",
    "        ys.append(v[2])\n",
    "        zs.append(v[1])\n",
    "        sizes.append([(len(sample_to_plot) - float(num))/len(sample_to_plot), 0, 0])\n",
    "ax.scatter(xs, ys, zs, c=sizes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
